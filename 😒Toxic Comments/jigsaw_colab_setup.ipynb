{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6720a3c6",
   "metadata": {},
   "source": [
    "# ðŸ“ Jigsaw Toxic Comment Classification (Colab Setup)\n",
    "\n",
    "This notebook sets up environment, config, dataset split, and class imbalance handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# ðŸ”¹ Colab Startup for Jigsaw Toxic Comment Project\n",
    "# ============================\n",
    "\n",
    "# 1. Install required libraries\n",
    "!pip install -U transformers scikit-learn pandas tqdm\n",
    "\n",
    "# 2. Import essentials\n",
    "import os\n",
    "from dataclasses import dataclass, replace\n",
    "from typing import Tuple\n",
    "from google.colab import drive\n",
    "\n",
    "# 3. Mount Google Drive (for persistent storage)\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 4. Set up directories\n",
    "DATA_DIR = \"/content/data\"                                      # dataset goes here\n",
    "CKPT_DIR = \"/content/drive/MyDrive/jigsaw_checkpoints\"          # checkpoints saved here\n",
    "ARTIFACTS_DIR = \"/content/drive/MyDrive/jigsaw_artifacts\"       # reports/metrics here\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(ARTIFACTS_DIR, \"errors\"), exist_ok=True)\n",
    "\n",
    "print(\"âœ… Directories ready!\")\n",
    "print(\"Data:\", DATA_DIR)\n",
    "print(\"Checkpoints:\", CKPT_DIR)\n",
    "print(\"Artifacts:\", ARTIFACTS_DIR)\n",
    "\n",
    "# 5. Full CFG dataclass\n",
    "@dataclass\n",
    "class CFG:\n",
    "    # ðŸ”¹ Model & training\n",
    "    model_name: str = \"roberta-base\"\n",
    "    max_length: int = 256\n",
    "    train_epochs: int = 5\n",
    "    train_batch_size: int = 16\n",
    "    eval_batch_size: int = 32\n",
    "    lr: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.1\n",
    "    grad_accum_steps: int = 1\n",
    "    amp: bool = True\n",
    "\n",
    "    # ðŸ”¹ Paths\n",
    "    data_dir: str = \"./data\"\n",
    "    ckpt_dir: str = \"./checkpoints\"\n",
    "    artifacts_dir: str = \"./artifacts\"\n",
    "    data_file: str = \"data.csv\"   # ðŸ‘ˆ name of your uploaded dataset\n",
    "\n",
    "    # ðŸ”¹ Reproducibility\n",
    "    seed: int = 42\n",
    "\n",
    "    # ðŸ”¹ Labels (multi-label classification targets)\n",
    "    labels: Tuple[str, ...] = (\n",
    "        \"toxic\",\n",
    "        \"severe_toxicity\",\n",
    "        \"obscene\",\n",
    "        \"threat\",\n",
    "        \"insult\",\n",
    "        \"identity_attack\",\n",
    "        \"sexual_explicit\",\n",
    "    )\n",
    "\n",
    "    # ðŸ”¹ Validation\n",
    "    val_size: float = 0.1\n",
    "    stratify_on_any: bool = True\n",
    "\n",
    "# 6. Instantiate config and update with Colab paths\n",
    "cfg = CFG()\n",
    "cfg = replace(cfg, data_dir=DATA_DIR, ckpt_dir=CKPT_DIR, artifacts_dir=ARTIFACTS_DIR)\n",
    "\n",
    "print(\"âœ… Config ready!\")\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ceaae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# ðŸ”¹ Data Load & Sanity Checks\n",
    "# ============================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure dataset exists in /content/data\n",
    "path = os.path.join(cfg.data_dir, cfg.data_file)\n",
    "assert os.path.exists(path), f\"Dataset not found at {path}\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist()[:12])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9bd710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# ðŸ”¹ Train/Val Split with Stratification\n",
    "# ============================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# any_toxic flag\n",
    "df[\"any_toxic\"] = (df[list(cfg.labels)].sum(axis=1) > 0).astype(int)\n",
    "\n",
    "stratify_vec = df[\"any_toxic\"] if cfg.stratify_on_any else None\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=cfg.val_size,\n",
    "    random_state=cfg.seed,\n",
    "    stratify=stratify_vec,\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "# prevalence helper\n",
    "def _prevalence(table, labels):\n",
    "    n = len(table)\n",
    "    stats = {col: float(table[col].sum())/max(n,1) for col in labels}\n",
    "    stats[\"any_toxic\"] = float(table[\"any_toxic\"].sum())/max(n,1)\n",
    "    return stats\n",
    "\n",
    "train_prev = _prevalence(train_df, cfg.labels)\n",
    "val_prev   = _prevalence(val_df, cfg.labels)\n",
    "\n",
    "prev_df = pd.DataFrame([\n",
    "    {\"split\": \"train\", **train_prev},\n",
    "    {\"split\": \"val\",   **val_prev},\n",
    "])\n",
    "prev_path = os.path.join(cfg.artifacts_dir, \"split_prevalence.csv\")\n",
    "prev_df.to_csv(prev_path, index=False)\n",
    "\n",
    "print(f\"Split sizes -> train: {len(train_df)} | val: {len(val_df)}\")\n",
    "print(f\"Saved label prevalence to {prev_path}\")\n",
    "prev_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# ðŸ”¹ Step 2: Class imbalance handling (pos_weight)\n",
    "# ============================\n",
    "\n",
    "# Ensure binary labels\n",
    "for _col in cfg.labels:\n",
    "    if train_df[_col].dtype != int and train_df[_col].dtype != bool:\n",
    "        train_df[_col] = (train_df[_col].astype(float) >= 0.5).astype(int)\n",
    "    if val_df[_col].dtype != int and val_df[_col].dtype != bool:\n",
    "        val_df[_col] = (val_df[_col].astype(float) >= 0.5).astype(int)\n",
    "\n",
    "N = len(train_df)\n",
    "pos_counts = {c: int(train_df[c].sum()) for c in cfg.labels}\n",
    "neg_counts = {c: int(N - pos_counts[c]) for c in cfg.labels}\n",
    "\n",
    "_eps = 1e-6\n",
    "pos_weight = {c: (neg_counts[c] / (pos_counts[c] + _eps)) for c in cfg.labels}\n",
    "\n",
    "posw_df = pd.DataFrame([\n",
    "    {\"label\": c, \"train_pos\": pos_counts[c], \"train_neg\": neg_counts[c], \"pos_weight\": pos_weight[c]}\n",
    "    for c in cfg.labels\n",
    "])\n",
    "posw_path = os.path.join(cfg.artifacts_dir, \"pos_weight.csv\")\n",
    "posw_df.to_csv(posw_path, index=False)\n",
    "print(f\"Saved pos_weight table to {posw_path}\")\n",
    "\n",
    "import torch\n",
    "pos_weight_tensor = torch.tensor([pos_weight[c] for c in cfg.labels], dtype=torch.float32)\n",
    "print(\"pos_weight_tensor:\", pos_weight_tensor)\n",
    "\n",
    "posw_df\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
